\section{Eigenvalues and Eigenvectors}

\subsection{Eigenvalues and EigenVectors}

\paragraph{Definition}
For \( T \in L(V, V) \), if \( T(\vv) = \alpha \vv \), then
\( \vv \) is an eigenvector of \( T \) corresponding to eigenvalue
\( \alpha \).

\paragraph{Eigenspace and Spectrum}
For eigenvalue \( \lambda \), the eigen space is the set
of all vectors thta correspond to that eigenvalue:
\( E_\lambda = \set{\vv \in V : T(\vv) = \lambda \vv} \).
The \textit{spectrum} of \( T \) is the set of all its
eigenvalues.
Observe that an eignespace is invariant under \( T \).

\paragraph{Independence of Eigenspaces}
For a linear \( T: V \to V \), \( E_\lambda(T) = \ker(\lambda I - T) \).
Further, if \( \lambda_1,\dots,\lambda_k \) are distinct eigenvalues,
then all their correponding eigenvectors \( \vv_1, \dots, \vv_k \)
are linearly independent.

Further, if \( \lambda \neq \mu \), then \( E_\lambda \cap E_\mu = \set{\zero} \).

\paragraph{Diagonalisability}
A matrix \( A \) is diagonalisable if there exists an invertible matrix
\( P \in GL(p, \mathbb{F}) \) such that \( D = P^{-1} D P \),
is a diagonal matrix. Alternatively, \( A = P D P^{-1} \).

A matrix is diagonalisable if and only if, its eigenvectors form a basis
for \( V \). Do note that the converse is not true. Diagonalisability
does not imply independent eigenvectors.

%
%
%
\subsection{The Characteristic Polynomial}

\paragraph{Definition}
For a \( p\times p \) matrix \( A \) over \( \mathbb{F} \), the characteristic
polynomial is \[
    \cp_A(t) = \det(tI - A).
\]
The eigenvalues of \( A \) are the roots of \( \cp_A(t) \).
In the above definition, the polynomial is monic but, it may be easier
to calculate the roots of \( \det(A - tI) \).

The characteristic polynomial is a similarity invariant.

\paragraph{Eigenvalues over Bases}
For a linear \( T: V \to V \), on a finite dimensional \( V \),
let \( \mathcal{B} \) be a basis for \( V \).
Let \( A \) be a matrix for \( T \) with respect to \( \mathcal{B} \).
Then, an eigenvector \( \vv \) for \( T \) with eigenvalue \( \lambda \)
is also an eigenvector for \( A \)
as \( {[\vv]}_\mathcal{B} \) with eigenvalue \( \lambda \).

\paragraph{Algorithm for Diagonalisation}
Given a matrix \( A \) with \( \dim V = n \),
\begin{enumerate}
    \item Calcualte \( \cp_A(T) \).
    \item Find all roots \( \alpha_i \) of the characteristic polynomial.
    \item For each root calculate \( N_i = \dim(E_{\alpha_i}(A) ) \) using
    \( \nullity(A - \alpha_i I) \).
    \item If \( \sum_{\alpha} N_i = n \) then, \( A \) is diagonalisable.
    \item \( P \) is the matrix with columns being the eigenvectors.
    \item \( D \) is a diagonal matrix with the eigenvalues on the diagonal
        that correspond to the eigenvectors in \( P \).
\end{enumerate}

%
%
%
\subsection{Multiplicities}

\paragraph{Geometric and Algebraic Multiplicities}
For a linear map \( T \), suppose that \( \lambda \)
is an eigenvalue of \( T \) so that \( t - \lambda \) is a factor
of the characteristic polynomial.
\begin{itemize}
    \item The \underline{geometric multiplicity} is \( \dim E_\lambda(T) \)
    \item The \underline{algebraic multiplicity} is the multiplicity of
        \( \lambda \) as a root of the characteristic polynomial.
\end{itemize}

\paragraph{Multiplicities, Determinant, Trace and Closure}
Observe that algebraic multiplicites will depend on the field. We prefer
\( \mathbb{C} \) as it is algebraically closed (polynomial of degree \( p \)
has \( p \) roots).
The for \( A \in \mathbb{M}_{p, p}(\mathbb{C}) \) with eigenvalues
\( \alpha_1, \dots, \alpha_n \) (counting non-distinct roots multiple times),
\[
    \det (A) = \prod_{i = 1}^n \alpha_i
    \quad\text{and}\quad
    \tr(A) = \sum_{i = 1}^n \alpha_i.
\]

\paragraph{Inequality of Geometric and Algebraic Multiplicities}
For an eigenvalue \( \lambda \), \[
    1 \leq \text{geometric multiplicity} \lambda \leq \text{algebraic}
.\]

\paragraph{Equivalencies on Multiplicities and Diagonalisability}
For a linear \( T: V \to V \) on a finite dimensional \( V \),
the following are equivalent:
\begin{itemize}
    \item \( T \) is diagonalisable.
    \item The sum of geometric multiplicites is equal to \( \dim V \).
    \item The eigenvectors of \( T \) form a basis for \( V \).
    \item \( V = E_{\lambda_1}(T) \oplus \cdots \oplus E_{\lambda_n}(T) \).
\end{itemize}

%
%
%
\subsection{Normal Operators}

\paragraph{Definition}
A linear map on an innner product space is normal if and only if
it commutes with its adjoint. For maps and matrices respective, that is \[
    T^* \circ T = T \circ T^*,  \quad A^* A = A A^*
.\]

Unitary, Hermetian, Orthogonal and Real-symmetric matrices are all normal.

\paragraph{Properties of Normal Maps}
For a normal map \( T \) on \( V \):
\begin{enumerate}
    \item The adjoint and map itself preserve the same length. That is,
        \( \norm*{T(\vv)} = \norm*{T^*(\vv)} \)
    \item For any scalar \( \alpha \), \( T - \alpha \) is normal.
    \item If \( \lambda \) is an eigenvalue, its conjugate \( \bar{\lambda} \)
        is an eigenvalue of \( T^* \).
        \item The eigenspaces are equivalent to the adjoint:
            \( E_\lambda(T) = E_{\bar{\lambda}}(T^*) \).
    \item All distrinct eigenspaces are orthogonal. That is if \( \lambda \neq \mu \),
        then \( E_\lambda(T) \perp E_\mu(T) \).
\end{enumerate}

\paragraph{Multiplicity of Normal Maps}
For a normal map \( T \) on \( V \), the algebraic multiplicity of
all eigenvalues is equivalent to its geometric multiplicity.
Recall that this is equivalent to diagonalisability.

\paragraph{The Spectral Theorem for Normal Maps}
For a finite dimensional inner product space over \( \mathcal{C} \),
with a normal \( T \in L(V, V) \), there exists an orthonormal basis
\( \mathcal{B} \) for \( V \) consisting of eigenvectors of \( T \).

Therefore, if \( A \in \mathbb{M}_{p, p}(\mathcal{C}) \) is normal,
\underline{there exists a unitary \( P \)} such that \[
    P^{-1} A P = P^* A P = D.
\]

Conversely, if \( {[T]}_{\mathcal{B}}^{\mathcal{B}} = A  PDP^{-1} \)
where \( P \) is unitary (thus \( \mathcal{B} \) is orthogonormal),
then both \( A \) and \( T \) are normal.

\paragraph{}
Suppose that \( A \) is normal with eigenvalues \( \lambda_i \).
Then there exist matrices \( P_i \) such that

\begin{align*}
    A = \sum_{i} \lambda_i P_i, &\quad\quad\quad P^2_i = P_i = P_i^*,   \\
    P_i P_j = 0 \quad i \neq j, &\quad\quad\quad \sum_{i} P_i = I.
\end{align*}

Each \( P_i \) is just the projection onto the orthgonal eigenspace \( E_{\lambda_i} \).

\paragraph{Self-Adjoint}
Recall that to be self-adjoint is just to be Hermetian or, in the real
case, symmetric.
\begin{enumerate}
    \item If \( T \) is self-adjoint then all its eigenvalues are real.
    \item If \( \mathbb{F} = \mathbb{C} \) and \( T \) is self-adjoint (Hermetian),
        then there is a unitary (orthonormal) basis for \( V \) 
        consisting of its eigenvectors.
    \item If \( \mathbb{F} = \mathbb{R} \) and \( T \) is self-adjoint (symmetric),
        then there is a real orthonormal basis for \( V \) consisting of its
        eigenvectors.
\end{enumerate}

%
%
%
\subsection{Conics and Quadric Surfaces}
TODO

%
%
%
\subsection{Unitary and Orthogonal Maps and Matrices}

\paragraph{Lengths of Eigenvalues of Unitary Maps}
Suppose that \( T \) is unitary. Then, all its eigenvalues lie on the
unit circle in \( \mathcal{C} \).
That is, for real \( \alpha_k \), the eigenvalues are \( e^{i a_k} \).
Also, \( V \) has a unitary base of eigenvectors of \( T \).

For \( \mathbb{F} = \mathbb{R} \), this implies that all eigenvalues
are \( \pm 1 \).

\paragraph{Rotation Matrix}
For a unitary \( T \), let \( \vv = \vx + i\vy \) for real \( \vx, \vy \).
Then by linearity (and omitting some steps), \[
    T(\vx + \vy) = (\cos \alpha \vx - \sin \alpha \vy) + i(\sin \alpha \vx + \cos \alpha \vy)
.\]

Then, assuming \( \set{\vx, \vy} \) is an orthonormal basis of the invariant
they span, we can restrict \( T \) to this 2D space and by using basis
\( \set{\vx, - \vy} \), we can see that \( T \) is a rotation by \( \alpha \)
with matrix \[
    R(\alpha) =
    \begin{pmatrix}
        \cos \alpha & - \sin \alpha \\
        \sin \alpha & \cos \alpha
    \end{pmatrix}
.\]

\paragraph{Characteristic Polynomial of Isometry}
Suppose that \( T \) is an isometry on a real inner product space
\( V \),
Then \[
    \cp_T(t)
    =
    \underbrace{{(t - 1)}^p {(t + 1)}^n}_{\text{real factors of 1, -1}}
    \underbrace{\prod_{j = 1}^{k} (t - e^{i \alpha_j}) (t - e^{-i\alpha_j})}_{\text{complex factors as conjugate pairs}}
\]
with \( p + n + 2k = \dim V \) and \( \alpha_j \in (0, \pi) \).

\subsubsection{Orthogonal Matrices in \(\mathbb{R}^3\)}

\paragraph{Variants of Orthogonal Matrices}
Suppose that \( A \) is a \( 3 \times 3 \) orthogonal matrix.
Then \( A A^T  = I \) implies that \( \det A \pm 1 \).
Thus, we split the group of orthogonal matrices \( O(3) \) into two groups:
\( SO(3) \) (subgroup with determinant 1) and, everything else.

Elements of \( SO(3) \) are similr to either the identity, \( (1) \oplus (-1) \oplus (-1) \)
or the rotation matrix: \[
    R(\theta) = \begin{pmatrix}
        1   &               &               \\
            & \cos \theta   & - \sin \theta \\
            & \sin \theta   & \cos \theta
    \end{pmatrix}, \theta \in (0, \pi)
.\]
Infact, being similar to the rotation is sufficient, if we let
\( \theta \in [0, \pi] \).

\paragraph{Finding Axis of Rotation}
Since \( \tr(A) = 1 + 2\cos \theta \), we can use this to find the angle
of rotation.
If \( 1 \) is the determiant then, \( E_1 (A) \) gives the axis of rotation.
If however \( -1 \) is the determinant then, \( E_{-1}(A) \) gives
us a vector normal to the plane of reflection and, the axis for the
subsequent rotation.

%
%
%
\subsection{Singular Value Decomposition}

\paragraph{Definition}
Let \( A \in \mathbb{M}_{p, q} (\mathbb{C}) \).
A \textit{singular value decomposition} of \( A \) is a factorisation
of the form \[
    A = U \Sigma V^*
\] where \( U, V \) are square and unitary and, \( \Sigma \)
is a \( p\times q \) matrix with zero off-diagonal terms and diagonal
terms called \textit{singular values} that satisfy \[
    \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_q > 0
.\]

The columns of \( U, V \) are called the left and right singular vectors
respectively.

This form always exists and is unique.

\paragraph{Preservation of Rank and Kernel under Composition with Adjoint}
Let \( A \) be a real or complex \( p\times q \) matrix.
Then the rank and kernel of \( A^* A \) is equal to the rank and kernel
of \( A \).

\paragraph{Constructing the SVD}

Let \( A \) be a \( p \times q \) matrix.  Then, \( A^* A  \) is Hermetian.

\begin{enumerate}
    \item Calculate \( W = A A^* \).
    \item Calculate the eigenvalues and vectors \( \lambda_i, \vv_i \)
        of \( A A^* \), ensuring that all \( \vv_i \) are normalised.
        Order the eigenvalues so that \[
            \lambda_1 \geq \lambda_2 \geq\cdots \geq \lambda_n \geq 0
        .\]
    \item \( V \) is the matrix with columns \( \vv_i \).
    \item Let \( \sigma_i = \sqrt(\lambda_i) \). Construct \( \Sigma \)
        with \( \sigma_i \) as the diagonal entries and then pad
        with zeros to make it a \( p \times q \) matrix.
    \item For \( i \in [1, k] \), define the columns of \( U \) as \[
        \vu_i = \frac{1}{\sigma_i} A \vv_i
    .\]
    If there is insufficient singular values, extend \( \vu_i \)
    to be an orthonormal basis for \( \mathbb{C}^p \).
    \item Then by this construction, \( U \Sigma = A V \) and thus
        \( A = U \Sigma V^* \) as required.
\end{enumerate}

\paragraph{Reduced Singular Value Decomposition}
Suppose that \( A \in \mathbb{M}_{p, q} (\mathbb{C}) \)
has a rank \( k \). Then we can choose to delete the last
\( q - k \) columns of \( V \), the last \( p - k \) columns
from \( U \) and avoid padding the zero columns of \( \Sigma \).
Then we get a reduced form \[
    A = \hat{U} \hat{\Sigma} \hat{V}^*
\]
where \( \hat{U} \) and \( \hat{V} \) are orthonormal
and \( \hat{\Sigma} \) is \( k\times k \), invertible
and diagonal.

\paragraph{Properties on the SVD}
\begin{enumerate}
    \item The lat \( q - k \) right singular vectors
    are an orthonormal basis for \( \ker A \).
    The first \( k \) left singular vectors are an orthonormal
    basis for \( \img A \).
    \item The sum of the squares of the singular values is
    equivalent to the trace of \( A^* A \).
\end{enumerate}

\paragraph{Pseudo-Inverse}
For any \( A \) with a reduced SVD, we can define a pseudo-inverse
as \[
    A^+ = \hat{V} \hat{\Sigma}^{-1} \hat{U}^*
.\]

\paragraph{Least Squares Solution with Pseudo}
The least-squares solution to \( A \vx = \vb \)
is given by \[
    \vx = A^+ \vb
.\]

\paragraph{Schur's Lemma}
Every matrix is unitarily similar to an upper triangular matrix.
We extend this for normal matrices, which are similar to
a diagonal matrix.
